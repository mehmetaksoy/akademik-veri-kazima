# Akademik Makale Toplama BetiÄŸi: Elmas TabanlÄ± Kuantum Teknolojileri

Bu proje, Google Colab Ã¼zerinde Ã§alÄ±ÅŸan bir Python betiÄŸidir ve [arXiv](https://arxiv.org/) ile [Semantic Scholar](https://www.semanticscholar.org/) platformlarÄ±ndan "elmas tabanlÄ± kuantum teknolojileri" Ã¼zerine odaklanan akademik makaleleri otomatik olarak toplamak iÃ§in tasarlanmÄ±ÅŸtÄ±r. Betik, toplanan makalelerin baÅŸlÄ±k, Ã¶zet, yazarlar, yayÄ±n tarihi, DOI ve PDF baÄŸlantÄ±sÄ± gibi temel metaverilerini Ã§Ä±karÄ±r ve bu verileri Google Drive'a hem JSON hem de CSV formatlarÄ±nda kaydeder.

## ğŸ¯ AmaÃ§

Bu projenin temel amacÄ±, belirli araÅŸtÄ±rma alanlarÄ±ndaki (bu durumda elmas tabanlÄ± kuantum teknolojileri) gÃ¼ncel akademik literatÃ¼rÃ¼ sistematik bir ÅŸekilde toplamak, dÃ¼zenlemek ve kolay eriÅŸilebilir bir formatta depolamaktÄ±r. Bu, literatÃ¼r taramasÄ± ve veri analizi sÃ¼reÃ§lerini otomatikleÅŸtirmeye yardÄ±mcÄ± olabilir.

## ğŸ› ï¸ KullanÄ±lan Teknolojiler ve KÃ¼tÃ¼phaneler

* **Programlama Dili:** Python
* **Ã‡alÄ±ÅŸma OrtamÄ±:** Google Colaboratory
* **Ana KÃ¼tÃ¼phaneler:**
    * `arxiv`: arXiv API'si ile etkileÅŸim iÃ§in.
    * `semanticscholar`: Semantic Scholar API'si ile etkileÅŸim iÃ§in.
    * `pandas`: Verileri CSV formatÄ±nda iÅŸlemek ve kaydetmek iÃ§in.
    * `json`: Verileri JSON formatÄ±nda iÅŸlemek ve kaydetmek iÃ§in.
    * `tqdm`: Veri toplama sÃ¼recinde ilerleme Ã§ubuklarÄ± gÃ¶stermek iÃ§in.
    * `google.colab.drive`: Google Drive entegrasyonu iÃ§in.
* **Veri KaynaklarÄ±:**
    * arXiv API
    * Semantic Scholar API

## âœ¨ Ã–zellikler

* **YapÄ±landÄ±rÄ±labilir Arama Terimleri:** Birden fazla arama terimi tanÄ±mlayarak geniÅŸ bir literatÃ¼r taramasÄ± yapabilme.
* **SonuÃ§ SÄ±nÄ±rlandÄ±rma:** Her arama terimi iÃ§in API'lerden Ã§ekilecek maksimum sonuÃ§ sayÄ±sÄ±nÄ± belirleyebilme.
* **Ã‡oklu Ã‡Ä±ktÄ± FormatÄ±:** Toplanan verileri hem JSON hem de CSV formatlarÄ±nda kaydedebilme.
* **Google Drive Entegrasyonu:** Toplanan verilerin doÄŸrudan kullanÄ±cÄ±nÄ±n Google Drive hesabÄ±na kaydedilmesi.
* **API Gecikme YÃ¶netimi:** API hÄ±z limitlerine takÄ±lmamak iÃ§in istekler arasÄ±na gecikme ekleyebilme.
* **Temel Veri Kalite Kontrolleri:** Toplanan veriler Ã¼zerinde benzersiz makale sayÄ±sÄ±, Ã¶zeti olmayan makaleler ve PDF baÄŸlantÄ±sÄ± olan makaleler gibi basit kontroller.
* **Hata YÃ¶netimi:** API istekleri sÄ±rasÄ±nda oluÅŸabilecek temel hatalarÄ± yakalama ve sÃ¼rece devam etme.

## âš™ï¸ Kurulum ve Ã‡alÄ±ÅŸtÄ±rma

Bu betik, Google Colab ortamÄ±nda Ã§alÄ±ÅŸtÄ±rÄ±lmak Ã¼zere tasarlanmÄ±ÅŸtÄ±r.

1.  **Google Colab OrtamÄ±:**
    * Bir Google hesabÄ±nÄ±zla [Google Colab](https://colab.research.google.com/) aÃ§Ä±n.
    * Bu `Veri_kazima001.ipynb` dosyasÄ±nÄ± Colab'e yÃ¼kleyin veya yeni bir notebook oluÅŸturup kodu kopyalayÄ±n.

2.  **KÃ¼tÃ¼phane Kurulumu:**
    * Notebook'un ilk kod hÃ¼cresi gerekli `arxiv` ve `semanticscholar` kÃ¼tÃ¼phanelerini `!pip install` komutu ile kuracaktÄ±r.

3.  **Google Drive BaÄŸlantÄ±sÄ±:**
    * Notebook, Google Drive'Ä±nÄ±za eriÅŸim izni isteyecektir. Bu izni vermeniz, toplanan verilerin Drive'Ä±nÄ±za kaydedilebilmesi iÃ§in gereklidir.

4.  **YapÄ±landÄ±rma (Ä°steÄŸe BaÄŸlÄ±):**
    * Notebook iÃ§indeki `CONFIG` sÃ¶zlÃ¼ÄŸÃ¼nÃ¼ dÃ¼zenleyerek arama yapÄ±lacak terimleri (`search_terms`), terim baÅŸÄ±na maksimum sonuÃ§ sayÄ±sÄ±nÄ± (`max_results_per_term`), Ã§Ä±ktÄ± formatlarÄ±nÄ± (`output_formats`) ve kaydedilecek dosya yolunu (`output_path`) deÄŸiÅŸtirebilirsiniz.
    ```python
    CONFIG = {
        "search_terms": [
            "diamond based quantum computing",
            "NV centers in diamond",
            # ... diÄŸer terimler
        ],
        "max_results_per_term": 150,
        "output_formats": ["json", "csv"],
        "output_path": "/content/drive/MyDrive/quantum_diamond_dataset",
        "api_delay": 1
    }
    ```

5.  **Ã‡alÄ±ÅŸtÄ±rma:**
    * TÃ¼m hÃ¼creleri sÄ±rayla Ã§alÄ±ÅŸtÄ±rÄ±n. Betik, veri toplama sÃ¼recini baÅŸlatacak, ilerlemeyi gÃ¶sterecek ve sonuÃ§larÄ± Google Drive'daki belirttiÄŸiniz yola kaydedecektir.

## ğŸ“ Ä°ÅŸ AkÄ±ÅŸÄ±

1.  Gerekli paketler kurulur ve Google Drive baÄŸlanÄ±r.
2.  KullanÄ±cÄ± tarafÄ±ndan tanÄ±mlanan arama terimleri ve diÄŸer yapÄ±landÄ±rma parametreleri yÃ¼klenir.
3.  **arXiv Veri Toplama:** Her bir arama terimi iÃ§in arXiv API'si kullanÄ±larak makaleler aranÄ±r ve ilgili metaveriler (ID, baÅŸlÄ±k, Ã¶zet, yazarlar, yayÄ±n tarihi, DOI, PDF URL) toplanÄ±r.
4.  **Semantic Scholar Veri Toplama:** Her bir arama terimi iÃ§in Semantic Scholar API'si kullanÄ±larak makaleler aranÄ±r ve ilgili metaveriler toplanÄ±r. (*Not: Mevcut betikte Semantic Scholar API'sinin `limit` parametresi ile ilgili bir sorun bulunmaktadÄ±r, aÅŸaÄŸÄ±ya bakÄ±nÄ±z.*)
5.  Toplanan veriler birleÅŸtirilir.
6.  Temel veri kalite kontrolleri yapÄ±lÄ±r (benzersiz makale sayÄ±sÄ±, Ã¶zeti olmayanlar, PDF linki olanlar).
7.  SonuÃ§lar, yapÄ±landÄ±rmada belirtilen formatlarda (JSON ve/veya CSV) Google Drive'a kaydedilir.
8.  Ä°ÅŸlem sonunda bir Ã¶zet raporu sunulur.

## âš ï¸ Bilinen Sorunlar ve Ä°yileÅŸtirme AlanlarÄ±

* **Semantic Scholar API Limiti:** BetiÄŸin mevcut haliyle Ã§alÄ±ÅŸtÄ±rÄ±lmasÄ± durumunda, Semantic Scholar API'sinden veri Ã§ekilirken "The limit parameter must be between 1 and 100 inclusive" hatasÄ± alÄ±nmaktadÄ±r. `CONFIG` iÃ§indeki `max_results_per_term` deÄŸeri (150) bu limite uymamaktadÄ±r.
    * **Ã‡Ã¶zÃ¼m Ã–nerisi:** `CONFIG["max_results_per_term"]` deÄŸerini Semantic Scholar iÃ§in 100 veya daha dÃ¼ÅŸÃ¼k bir deÄŸere ayarlayÄ±n veya Semantic Scholar API'sinin sayfalama (pagination) Ã¶zelliÄŸini destekleyip desteklemediÄŸini kontrol ederek daha fazla sonuÃ§ Ã§ekmek iÃ§in dÃ¶ngÃ¼sel istekler gÃ¶nderin.
* **arXiv Deprecation UyarÄ±sÄ±:** `arxiv` kÃ¼tÃ¼phanesinin `Search.results` metodu kullanÄ±mdan kaldÄ±rÄ±lmÄ±ÅŸtÄ±r (`DeprecationWarning`). Bunun yerine `Client.results` kullanÄ±lmasÄ± Ã¶nerilmektedir. KÃ¼tÃ¼phane gÃ¼ncellenerek veya kodda ilgili deÄŸiÅŸiklik yapÄ±larak bu uyarÄ± giderilebilir.
* **Yinelenen KayÄ±tlar:** Betik, farklÄ± arama terimleri veya farklÄ± API'lerden aynÄ± makalenin birden fazla kez Ã§ekilmesine neden olabilir. Toplam 700 kayÄ±t toplanmasÄ±na raÄŸmen benzersiz makale sayÄ±sÄ±nÄ±n 344 olmasÄ± buna iÅŸaret etmektedir. Kaydetmeden Ã¶nce veya sonra daha kapsamlÄ± bir yinelenen kayÄ±t temizleme adÄ±mÄ± eklenebilir (Ã¶rneÄŸin, DOI veya baÅŸlÄ±k+yazar kombinasyonuna gÃ¶re).
* **Ã–zet UzunluÄŸu KontrolÃ¼:** Ã–zeti olmayan makaleler iÃ§in yapÄ±lan kontrol (`len(p['abstract'])<100`) bazÄ± kÄ±sa ama geÃ§erli Ã¶zetleri de filtreleyebilir. Bu eÅŸik deÄŸeri ihtiyaca gÃ¶re ayarlanabilir veya daha sofistike bir boÅŸ Ã¶zet kontrolÃ¼ eklenebilir.
* **Hata YÃ¶netimi:** API'lerden gelen hatalar temel `try-except` bloklarÄ± ile yakalanmaktadÄ±r. Daha detaylÄ± hata loglamasÄ± ve farklÄ± hata tÃ¼rlerine gÃ¶re Ã¶zel aksiyonlar (Ã¶rn: geÃ§ici API sorunlarÄ±nda yeniden deneme mekanizmasÄ±) eklenebilir.

## ğŸ“„ Ã‡Ä±ktÄ± DosyalarÄ±

## ğŸ“ Ä°letiÅŸim

ğŸ› **Bug Report**: GitHub Issues kullanÄ±n  
ğŸ’¡ **Feature Request**: Discussions bÃ¶lÃ¼mÃ¼nden Ã¶nerinizi paylaÅŸÄ±n  
ğŸ“§ E-posta: [mehmetaksoy49@gmail.com]

- Pull Request ile katkÄ±da bulunun
- Projeyi yÄ±ldÄ±zlamayÄ± unutmayÄ±n! â­

---

**Not**: Bu proje eÄŸitim amaÃ§lÄ± geliÅŸtirilmiÅŸtir ve akademik Ã§alÄ±ÅŸmalarda referans olarak kullanÄ±labilir.

Betik baÅŸarÄ±yla tamamlandÄ±ÄŸÄ±nda, Google Drive'Ä±nÄ±zda `CONFIG["output_path"]` ile belirtilen konumda aÅŸaÄŸÄ±daki dosyalar oluÅŸturulur:

* `quantum_diamond_dataset.json`: Toplanan tÃ¼m makale verilerini iÃ§eren JSON dosyasÄ±.
* `quantum_diamond_dataset.csv`: Toplanan tÃ¼m makale verilerini iÃ§eren CSV dosyasÄ± (yazar listeleri noktalÄ± virgÃ¼lle ayrÄ±lmÄ±ÅŸ string olarak kaydedilir).
