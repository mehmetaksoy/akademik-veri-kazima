{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTulqdvOpRQr",
        "outputId": "f4b76876-dbca-4120-f223-a1b26c9e4fa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Mounted at /content/drive\n",
            "\n",
            "==================================================\n",
            "VERİ TOPLAMA SÜRECİ BAŞLATILIYOR\n",
            "==================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rarXiv Veri Toplama:   0%|          | 0/5 [00:00<?, ?it/s]<ipython-input-1-d75d993dabee>:50: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
            "  for result in search.results():\n",
            "arXiv Veri Toplama:  40%|████      | 2/5 [00:28<00:46, 15.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "arXiv hatası (NV centers in diamond): Page of results was unexpectedly empty (https://export.arxiv.org/api/query?search_query=NV+centers+in+diamond&id_list=&sortBy=submittedDate&sortOrder=descending&start=100&max_results=100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "arXiv Veri Toplama: 100%|██████████| 5/5 [00:50<00:00, 10.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "arXiv'den toplanan makale sayısı: 700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Semantic Scholar Veri Toplama: 100%|██████████| 5/5 [00:00<00:00, 4134.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Semantic Scholar hatası (diamond based quantum computing): The limit parameter must be between 1 and 100 inclusive.\n",
            "\n",
            "Semantic Scholar hatası (NV centers in diamond): The limit parameter must be between 1 and 100 inclusive.\n",
            "\n",
            "Semantic Scholar hatası (quantum sensors with diamonds): The limit parameter must be between 1 and 100 inclusive.\n",
            "\n",
            "Semantic Scholar hatası (spin qubits diamond): The limit parameter must be between 1 and 100 inclusive.\n",
            "\n",
            "Semantic Scholar hatası (diamonds in quantum optics): The limit parameter must be between 1 and 100 inclusive.\n",
            "Semantic Scholar'dan toplanan makale sayısı: 0\n",
            "\n",
            "Toplam toplanan makale sayısı: 700\n",
            "\n",
            "VERİ KALİTE KONTROLLERİ:\n",
            "- Benzersiz makale sayısı: 344\n",
            "- Özeti olmayan makaleler: 0\n",
            "- PDF bağlantısı olanlar: 700\n",
            "\n",
            "VERİ KAYIT İŞLEMLERİ:\n",
            "JSON dosyası kaydedildi: /content/drive/MyDrive/quantum_diamond_dataset.json\n",
            "CSV dosyası kaydedildi: /content/drive/MyDrive/quantum_diamond_dataset.csv\n",
            "\n",
            "==================================================\n",
            "VERİ TOPLAMA SÜRECİ TAMAMLANDI\n",
            "==================================================\n",
            "Toplam Kayıt Sayısı: 700\n",
            "Kayıtlar Google Drive'da şu konuma kaydedildi: /content/drive/MyDrive/quantum_diamond_dataset[.json/.csv]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Google Colab için Tam Entegre Veri Toplama Sistemi\n",
        "\"\"\"\n",
        "\n",
        "# 1. GEREKLİ PAKETLERİN KURULUMU\n",
        "!pip install arxiv semanticscholar --quiet  # Sessiz kurulum için --quiet parametresi\n",
        "\n",
        "# 2. GOOGLE DRIVE ENTEGRASYONU\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 3. KÜTÜPHANELERİN YÜKLENMESİ\n",
        "import arxiv\n",
        "import json\n",
        "import pandas as pd\n",
        "from semanticscholar import SemanticScholar\n",
        "from tqdm import tqdm  # İlerleme çubuğu için\n",
        "import time  # API istekleri arası bekleme süresi\n",
        "\n",
        "# 4. YAPILANDIRMA PARAMETRELERİ\n",
        "CONFIG = {\n",
        "    \"search_terms\": [\n",
        "        \"diamond based quantum computing\",\n",
        "        \"NV centers in diamond\",\n",
        "        \"quantum sensors with diamonds\",\n",
        "        \"spin qubits diamond\",\n",
        "        \"diamonds in quantum optics\"\n",
        "    ],\n",
        "    \"max_results_per_term\": 150,  # Her terim için maks sonuç\n",
        "    \"output_formats\": [\"json\", \"csv\"],  # Çıktı formatları\n",
        "    \"output_path\": \"/content/drive/MyDrive/quantum_diamond_dataset\",\n",
        "    \"api_delay\": 1  # API istekleri arası bekleme süresi (saniye)\n",
        "}\n",
        "\n",
        "# 5. VERİ TOPLAMA FONKSİYONLARI\n",
        "\n",
        "def get_arxiv_data():\n",
        "    \"\"\"arXiv API'den veri çeker\"\"\"\n",
        "    collected = []\n",
        "\n",
        "    for term in tqdm(CONFIG[\"search_terms\"], desc=\"arXiv Veri Toplama\"):\n",
        "        try:\n",
        "            search = arxiv.Search(\n",
        "                query=term,\n",
        "                max_results=CONFIG[\"max_results_per_term\"],\n",
        "                sort_by=arxiv.SortCriterion.SubmittedDate\n",
        "            )\n",
        "\n",
        "            for result in search.results():\n",
        "                collected.append({\n",
        "                    \"id\": result.get_short_id(),\n",
        "                    \"title\": result.title,\n",
        "                    \"abstract\": result.summary,\n",
        "                    \"authors\": [a.name for a in result.authors],\n",
        "                    \"published\": result.published.isoformat() if result.published else None,\n",
        "                    \"doi\": result.doi,\n",
        "                    \"pdf_url\": result.pdf_url,\n",
        "                    \"source\": \"arXiv\"\n",
        "                })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\narXiv hatası ({term}): {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return collected\n",
        "\n",
        "def get_semantic_scholar_data():\n",
        "    \"\"\"Semantic Scholar API'den veri çeker\"\"\"\n",
        "    collected = []\n",
        "    scholar = SemanticScholar()\n",
        "\n",
        "    for term in tqdm(CONFIG[\"search_terms\"], desc=\"Semantic Scholar Veri Toplama\"):\n",
        "        try:\n",
        "            results = scholar.search_paper(\n",
        "                term,\n",
        "                limit=CONFIG[\"max_results_per_term\"],\n",
        "                fields=[\"title\", \"abstract\", \"authors\", \"publicationDate\", \"openAccessPdf\", \"externalIds\"]\n",
        "            )\n",
        "\n",
        "            time.sleep(CONFIG[\"api_delay\"])  # API rate limit koruması\n",
        "\n",
        "            for paper in results.data:\n",
        "                # Yazar isimlerini işleme\n",
        "                authors = [a['author']['name'] for a in paper['authors']] if paper.get('authors') else []\n",
        "\n",
        "                # PDF URL kontrolü\n",
        "                pdf_url = paper['openAccessPdf']['url'] if paper.get('openAccessPdf') else None\n",
        "\n",
        "                # DOI bilgisi\n",
        "                doi = paper['externalIds'].get('DOI') if paper.get('externalIds') else None\n",
        "\n",
        "                collected.append({\n",
        "                    \"id\": paper['paperId'],\n",
        "                    \"title\": paper.get('title', 'No Title'),\n",
        "                    \"abstract\": paper.get('abstract', 'No Abstract'),\n",
        "                    \"authors\": authors,\n",
        "                    \"published\": paper.get('publicationDate'),\n",
        "                    \"doi\": doi,\n",
        "                    \"pdf_url\": pdf_url,\n",
        "                    \"source\": \"Semantic Scholar\"\n",
        "      22 A3          })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nSemantic Scholar hatası ({term}): {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return collected\n",
        "\n",
        "# 6. VERİ TOPLAMA İŞLEMLERİ\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"VERİ TOPLAMA SÜRECİ BAŞLATILIYOR\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "# arXiv verilerini topla\n",
        "arxiv_data = get_arxiv_data()\n",
        "print(f\"\\narXiv'den toplanan makale sayısı: {len(arxiv_data)}\")\n",
        "\n",
        "# Semantic Scholar verilerini topla\n",
        "semantic_data = get_semantic_scholar_data()\n",
        "print(f\"Semantic Scholar'dan toplanan makale sayısı: {len(semantic_data)}\")\n",
        "\n",
        "# Tüm verileri birleştir\n",
        "combined_data = arxiv_data + semantic_data\n",
        "print(f\"\\nToplam toplanan makale sayısı: {len(combined_data)}\")\n",
        "\n",
        "# 7. VERİ KALİTE KONTROLLERİ\n",
        "print(\"\\nVERİ KALİTE KONTROLLERİ:\")\n",
        "print(f\"- Benzersiz makale sayısı: {len({p['id'] for p in combined_data})}\")\n",
        "print(f\"- Özeti olmayan makaleler: {sum(1 for p in combined_data if len(p['abstract'])<100)}\")\n",
        "print(f\"- PDF bağlantısı olanlar: {sum(1 for p in combined_data if p['pdf_url'])}\")\n",
        "\n",
        "# 8. VERİLERİ KAYDETME\n",
        "print(\"\\nVERİ KAYIT İŞLEMLERİ:\")\n",
        "\n",
        "# JSON kaydetme\n",
        "if \"json\" in CONFIG[\"output_formats\"]:\n",
        "    json_path = f\"{CONFIG['output_path']}.json\"\n",
        "    with open(json_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(combined_data, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"JSON dosyası kaydedildi: {json_path}\")\n",
        "\n",
        "# CSV kaydetme\n",
        "if \"csv\" in CONFIG[\"output_formats\"]:\n",
        "    csv_path = f\"{CONFIG['output_path']}.csv\"\n",
        "    df = pd.DataFrame(combined_data)\n",
        "\n",
        "    # Yazar listelerini string'e çevir\n",
        "    df['authors'] = df['authors'].apply(lambda x: '; '.join(x) if x else '')\n",
        "\n",
        "    df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"CSV dosyası kaydedildi: {csv_path}\")\n",
        "\n",
        "# 9. SONUÇ RAPORU\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"VERİ TOPLAMA SÜRECİ TAMAMLANDI\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Toplam Kayıt Sayısı: {len(combined_data)}\")\n",
        "print(f\"Kayıtlar Google Drive'da şu konuma kaydedildi: {CONFIG['output_path']}[.json/.csv]\")"
      ]
    }
  ]
}